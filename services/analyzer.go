package services

import (
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"strings"
	"text-analyzer/models"
)

// AIClient ‚Äî –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ª—é–±–æ–≥–æ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (OpenRouter, Groq)
type AIClient interface {
	Analyze(text string) (string, *models.TokenUsage, error)
}

type AnalyzerService struct {
	client       AIClient
	fetcher      *ContentFetcher
	serper       *SerperClient
	promptConfig *PromptConfig
}

func NewAnalyzerService(client AIClient, fetcher *ContentFetcher, serper *SerperClient, promptConfig *PromptConfig) *AnalyzerService {
	return &AnalyzerService{
		client:       client,
		fetcher:      fetcher,
		serper:       serper,
		promptConfig: promptConfig,
	}
}

// NewAnalyzerServiceGroq ‚Äî –∞–ª–∏–∞—Å –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (—Ç–æ—Ç –∂–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä)
func NewAnalyzerServiceGroq(client *GroqClient, fetcher *ContentFetcher, serper *SerperClient, promptConfig *PromptConfig) *AnalyzerService {
	return NewAnalyzerService(client, fetcher, serper, promptConfig)
}

func (s *AnalyzerService) AnalyzeText(text string, progress ...func(string)) (*models.AnalysisResponse, error) {
	report := func(msg string) {
		log.Printf("[ANALYZER] %s", msg)
		if len(progress) > 0 && progress[0] != nil {
			progress[0](msg)
		}
	}
	report(fmt.Sprintf("üìÑ –ß–∏—Ç–∞—é —Ç–µ–∫—Å—Ç... %d —Å–∏–º–≤–æ–ª–æ–≤", len(text)))

	var searchContext string
	if s.serper != nil && s.serper.APIKey != "" {
		report("üîç –ò—â—É —Ñ–∞–∫—Ç—ã –ø–æ —Ç–µ–º–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ...")
		searchResults, err := s.serper.SearchForFactCheck(text)
		if err != nil {
			report("‚ö† –ü–æ–∏—Å–∫ –≤ —Å–µ—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–µ–≥–æ")
		} else if searchResults != "" {
			searchContext = "\n\n--- –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ò–ù–¢–ï–†–ù–ï–¢–ê –î–õ–Ø –ü–†–û–í–ï–†–ö–ò –§–ê–ö–¢–û–í ---\n" + searchResults
			report("‚úì –ù–∞—à—ë–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–µ—Ç–∏")
		} else {
			report("‚ö† –ü–æ —Ç–µ–º–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å, –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
		}
	}

	report(fmt.Sprintf("üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç –Ω–∞ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é... (%d —Å–∏–º–≤.)", len(text)+len(searchContext)))
	report("‚è≥ –ü—Ä–æ–≤–µ—Ä—è—é –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –ª–æ–≥–∏–∫—É –∏ —Ñ–∞–∫—Ç—ã...")

	fullText := text + searchContext
	rawResponse, tokenUsage, err := s.client.Analyze(fullText)
	if err != nil {
		report(fmt.Sprintf("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: %v", err))
		return nil, err
	}

	report("üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
	if tokenUsage != nil {
		report(fmt.Sprintf("üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: %d (–∑–∞–ø—Ä–æ—Å: %d, –æ—Ç–≤–µ—Ç: %d)", 
			tokenUsage.TotalTokens, tokenUsage.PromptTokens, tokenUsage.CompletionTokens))
	}

	jsonStr := extractJSON(rawResponse)
	jsonStr = fixJSONTypes(jsonStr)

	var response models.AnalysisResponse
	if err := json.Unmarshal([]byte(jsonStr), &response); err != nil {
		cleanJSON := strings.ReplaceAll(jsonStr, "\n", " ")
		cleanJSON = strings.ReplaceAll(cleanJSON, "\t", " ")
		if err := json.Unmarshal([]byte(cleanJSON), &response); err != nil {
			report("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
			return &models.AnalysisResponse{
				Summary:     "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç",
				RawResponse: rawResponse,
			}, nil
		}
	}

	response.RawResponse = rawResponse
	response.Usage = tokenUsage

	report(fmt.Sprintf("üìä –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: %d/10 ¬∑ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π: %d ¬∑ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫: %d",
		response.CredibilityScore, len(response.Manipulations), len(response.LogicalIssues)))
	if response.CredibilityScore <= 3 {
		report("üî¥ –í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
	} else if response.CredibilityScore <= 6 {
		report("üü° –ö–æ–Ω—Ç–µ–Ω—Ç –≤—ã–∑—ã–≤–∞–µ—Ç —Å–æ–º–Ω–µ–Ω–∏—è")
	} else {
		report("üü¢ –ö–æ–Ω—Ç–µ–Ω—Ç –≤—ã–≥–ª—è–¥–∏—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ")
	}

	if response.CredibilityScore <= 7 && s.serper != nil && s.serper.APIKey != "" {
		report("üîé –ü—Ä–æ–≤–µ—Ä—è—é –ø–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º...")
		verification, err := s.verifyAndFindTruth(text, &response)
		if err != nil {
			report("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
		} else {
			response.Verification = *verification
			if verification.IsFake {
				report(fmt.Sprintf("üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (%d)", len(verification.FakeReasons)))
			} else {
				report("‚úì –ü–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
			}
		}
	}

	report("‚úÖ –ì–æ—Ç–æ–≤–æ!")
	return &response, nil
}

func (s *AnalyzerService) AnalyzeURL(url string, progress ...func(string)) (*models.AnalysisResponse, error) {
	report := func(msg string) {
		log.Printf("[ANALYZER] %s", msg)
		if len(progress) > 0 && progress[0] != nil {
			progress[0](msg)
		}
	}

	report("üåê –ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")

	content, err := s.fetcher.FetchURL(url)
	if err != nil {
		report(fmt.Sprintf("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: %v", err))
		return nil, err
	}

	report(fmt.Sprintf("‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —á–∏—Ç–∞—é –∫–æ–Ω—Ç–µ–Ω—Ç... (%d —Å–∏–º–≤–æ–ª–æ–≤)", len(content)))
	report("üî¨ –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ...")

	var progressFn func(string)
	if len(progress) > 0 {
		progressFn = progress[0]
	}
	response, err := s.AnalyzeText(content, progressFn)
	if err != nil {
		return nil, err
	}

	response.SourceURL = url
	return response, nil
}

func extractJSON(text string) string {
	// –ò—â–µ–º JSON –º–µ–∂–¥—É ```json –∏ ``` –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ { –∏ }
	
	// –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ markdown –±–ª–æ–∫–µ
	if strings.Contains(text, "```json") {
		start := strings.Index(text, "```json")
		if start != -1 {
			start += 7 // –¥–ª–∏–Ω–∞ "```json"
			end := strings.Index(text[start:], "```")
			if end != -1 {
				return strings.TrimSpace(text[start : start+end])
			}
		}
	}
	
	// –ò—â–µ–º –ø–µ—Ä–≤—ã–π { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π }
	start := strings.Index(text, "{")
	end := strings.LastIndex(text, "}")
	
	if start != -1 && end != -1 && end > start {
		jsonStr := text[start : end+1]
		
		// –û—á–∏—â–∞–µ–º –æ—Ç escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
		jsonStr = strings.ReplaceAll(jsonStr, "\\n", " ")
		jsonStr = strings.ReplaceAll(jsonStr, "\\t", " ")
		jsonStr = strings.ReplaceAll(jsonStr, "\\\"", "\"")
		
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON
		var testMap map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &testMap); err == nil {
			return jsonStr
		}
		
		log.Printf("[PARSER] ‚ö† JSON –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –≤–æ–∑–≤—Ä–∞—â–∞—é –∫–∞–∫ –µ—Å—Ç—å")
		return jsonStr
	}
	
	return text
}


// verifyAndFindTruth - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—å—é –∏ –∏—â–µ—Ç –Ω–∞—Å—Ç–æ—è—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
func (s *AnalyzerService) verifyAndFindTruth(text string, analysis *models.AnalysisResponse) (*models.Verification, error) {
	log.Printf("[VERIFIER] üîç –ù–∞—á–∏–Ω–∞—é –≥–ª—É–±–æ–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é...")
	
	verification := &models.Verification{
		IsFake:      analysis.CredibilityScore <= 5,
		FakeReasons: []string{},
	}
	
	// –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã –ø–æ—á–µ–º—É —Å—Ç–∞—Ç—å—è —Ñ–∞–ª—å—à–∏–≤–∞—è
	if len(analysis.Manipulations) > 0 {
		verification.FakeReasons = append(verification.FakeReasons, 
			fmt.Sprintf("–ù–∞–π–¥–µ–Ω–æ %d –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏ –ø—Ä–∏–µ–º–æ–≤ –¥–µ–º–∞–≥–æ–≥–∏–∏", len(analysis.Manipulations)))
	}
	
	if len(analysis.LogicalIssues) > 0 {
		verification.FakeReasons = append(verification.FakeReasons,
			fmt.Sprintf("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ %d –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π", len(analysis.LogicalIssues)))
	}
	
	if len(analysis.FactCheck.MissingEvidence) > 0 {
		verification.FakeReasons = append(verification.FakeReasons,
			fmt.Sprintf("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è %d —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π", len(analysis.FactCheck.MissingEvidence)))
	}
	
	if len(analysis.FactCheck.OpinionsAsFacts) > 0 {
		verification.FakeReasons = append(verification.FakeReasons,
			fmt.Sprintf("–ú–Ω–µ–Ω–∏—è –≤—ã–¥–∞—é—Ç—Å—è –∑–∞ —Ñ–∞–∫—Ç—ã: %d —Å–ª—É—á–∞–µ–≤", len(analysis.FactCheck.OpinionsAsFacts)))
	}
	
	// –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
	keywords := extractMainClaims(text, analysis)
	if len(keywords) == 0 {
		log.Printf("[VERIFIER] ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è")
		return verification, nil
	}
	
	log.Printf("[VERIFIER] üîë –ö–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: %v", keywords)
	
	// –ò—â–µ–º –Ω–∞—Å—Ç–æ—è—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
	var allResults []string
	var verifiedSources []models.Source
	
	for i, claim := range keywords {
		if i >= 3 { // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 –∑–∞–ø—Ä–æ—Å–∞–º–∏
			break
		}
		
		log.Printf("[VERIFIER] üåê –ü—Ä–æ–≤–µ—Ä—è—é —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ %d: %s", i+1, claim)
		
		results, err := s.serper.SearchMultiLanguage(claim)
		if err != nil {
			log.Printf("[VERIFIER] ‚ö† –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: %v", err)
			continue
		}
		
		if len(results) > 0 {
			log.Printf("[VERIFIER] ‚úì –ù–∞–π–¥–µ–Ω–æ %d —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", len(results))
			
			// –ë–µ—Ä–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
			for j, result := range results {
				if j >= 3 {
					break
				}
				
				allResults = append(allResults, fmt.Sprintf(
					"‚Ä¢ %s\n  –ò—Å—Ç–æ—á–Ω–∏–∫: %s\n  %s",
					result.Title, result.Link, result.Snippet,
				))
				
				verifiedSources = append(verifiedSources, models.Source{
					Title:       result.Title,
					URL:         result.Link,
					Description: result.Snippet,
				})
			}
		}
	}
	
	if len(allResults) > 0 {
		verification.RealInformation = "–ù–ê–°–¢–û–Ø–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ü–†–û–í–ï–†–ï–ù–ù–´–• –ò–°–¢–û–ß–ù–ò–ö–û–í:\n\n" + 
			strings.Join(allResults, "\n\n")
		verification.VerifiedSources = verifiedSources
		
		log.Printf("[VERIFIER] ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –Ω–∞—Å—Ç–æ—è—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(verifiedSources))
	} else {
		verification.RealInformation = "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –∏–∑ —Å—Ç–∞—Ç—å–∏."
		log.Printf("[VERIFIER] ‚ö† –ù–∞—Å—Ç–æ—è—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
	}
	
	return verification, nil
}

// extractMainClaims - –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ AI (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞–¥ —Å—ã—Ä—ã–º —Ç–µ–∫—Å—Ç–æ–º)
func extractMainClaims(text string, analysis *models.AnalysisResponse) []string {
	claims := []string{}
	seen := make(map[string]bool)

	addUnique := func(s string) {
		s = strings.TrimSpace(s)
		if len(s) > 15 && len(s) < 250 && !seen[s] {
			seen[s] = true
			claims = append(claims, s)
		}
	}

	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: —Ñ–∞–∫—Ç—ã –∏–∑ missing_evidence ‚Äî —ç—Ç–æ —Å–∞–º—ã–µ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
	for _, fact := range analysis.FactCheck.MissingEvidence {
		addUnique(fact)
		if len(claims) >= 2 {
			break
		}
	}

	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ AI-–∞–Ω–∞–ª–∏–∑–∞
	for _, fact := range analysis.FactCheck.VerifiableFacts {
		addUnique(fact)
		if len(claims) >= 4 {
			break
		}
	}

	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –º–Ω–µ–Ω–∏—è, –ø–æ–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ —Ñ–∞–∫—Ç—ã
	for _, op := range analysis.FactCheck.OpinionsAsFacts {
		addUnique(op)
		if len(claims) >= 5 {
			break
		}
	}

	// Fallback: –µ—Å–ª–∏ AI –Ω–µ –¥–∞–ª —Ñ–∞–∫—Ç–æ–≤, –∏—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —á–∏—Å–ª–∞–º–∏/–∏–º–µ–Ω–∞–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ
	if len(claims) == 0 {
		reNumbers := regexp.MustCompile(`\d`)
		sentences := strings.Split(text, ".")
		for _, sentence := range sentences {
			sentence = strings.TrimSpace(sentence)
			// –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (–∏–º–µ–Ω–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)
			if len(sentence) > 30 && len(sentence) < 200 && reNumbers.MatchString(sentence) {
				addUnique(sentence)
				if len(claims) >= 3 {
					break
				}
			}
		}
		// –ï—Å–ª–∏ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
		if len(claims) == 0 {
			for _, sentence := range sentences {
				sentence = strings.TrimSpace(sentence)
				if len(sentence) > 30 && len(sentence) < 200 {
					addUnique(sentence)
					if len(claims) >= 3 {
						break
					}
				}
			}
		}
	}

	return claims
}


// fixJSONTypes –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ JSON (—Å—Ç—Ä–æ–∫–∏ -> —á–∏—Å–ª–∞/bool)
func fixJSONTypes(jsonStr string) string {
	// –ò—Å–ø—Ä–∞–≤–ª—è–µ–º credibility_score: "1" -> 1 –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ –±–µ–∑ –∫–∞–≤—ã—á–µ–∫
	re := regexp.MustCompile(`"credibility_score"\s*:\s*"?(\d+)"?`)
	jsonStr = re.ReplaceAllString(jsonStr, `"credibility_score": $1`)
	
	// –ò—Å–ø—Ä–∞–≤–ª—è–µ–º is_fake: "true" -> true, "false" -> false
	jsonStr = strings.ReplaceAll(jsonStr, `"is_fake": "true"`, `"is_fake": true`)
	jsonStr = strings.ReplaceAll(jsonStr, `"is_fake": "false"`, `"is_fake": false`)
	jsonStr = strings.ReplaceAll(jsonStr, `"is_fake": true`, `"is_fake": true`) // —É–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
	jsonStr = strings.ReplaceAll(jsonStr, `"is_fake": false`, `"is_fake": false`) // —É–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
	
	return jsonStr
}
