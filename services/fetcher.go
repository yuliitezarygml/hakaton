package services

import (
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"regexp"
	"strings"
	"time"

	"golang.org/x/net/html"
)

type ContentFetcher struct{}

func NewContentFetcher() *ContentFetcher {
	return &ContentFetcher{}
}

func isFacebookURL(u string) bool {
	return strings.Contains(u, "facebook.com/") || strings.Contains(u, "fb.com/") || strings.Contains(u, "fb.watch/")
}

// toMbasic converts a facebook.com URL to mbasic.facebook.com for lightweight HTML scraping.
func toMbasic(u string) string {
	u = strings.Replace(u, "https://www.facebook.com", "https://mbasic.facebook.com", 1)
	u = strings.Replace(u, "https://facebook.com", "https://mbasic.facebook.com", 1)
	u = strings.Replace(u, "https://m.facebook.com", "https://mbasic.facebook.com", 1)
	// Strip tracking params that can cause redirects to login
	if idx := strings.Index(u, "?"); idx != -1 {
		// Keep the base URL only ‚Äî tracking params break mbasic
		u = u[:idx]
	}
	// Remove trailing # fragments
	if idx := strings.Index(u, "#"); idx != -1 {
		u = u[:idx]
	}
	return u
}

func (f *ContentFetcher) FetchURL(url string) (string, error) {
	log.Printf("[FETCHER] üåê –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å URL: %s", url)

	// Facebook requires special handling via mbasic.facebook.com
	if isFacebookURL(url) {
		return f.fetchFacebook(url)
	}

	client := &http.Client{Timeout: 30 * time.Second}

	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return "", fmt.Errorf("–æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: %w", err)
	}

	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
	req.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
	req.Header.Set("Accept-Language", "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7")

	log.Printf("[FETCHER] üì° –û—Ç–ø—Ä–∞–≤–ª—è—é HTTP –∑–∞–ø—Ä–æ—Å...")
	resp, err := client.Do(req)
	if err != nil {
		return "", fmt.Errorf("–æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: %w", err)
	}
	defer resp.Body.Close()

	log.Printf("[FETCHER] ‚úì –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: —Å—Ç–∞—Ç—É—Å %d", resp.StatusCode)
	contentType := resp.Header.Get("Content-Type")
	log.Printf("[FETCHER] üìÑ Content-Type: %s", contentType)

	// –ë–ª–æ–∫–∏—Ä—É–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã ‚Äî —Ç–æ–ª—å–∫–æ HTML/—Ç–µ–∫—Å—Ç –º–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
	ct := strings.ToLower(contentType)
	blockedTypes := []string{
		"application/pdf",
		"application/msword",
		"application/vnd.openxmlformats",
		"application/vnd.ms-",
		"application/zip",
		"application/octet-stream",
		"image/",
		"video/",
		"audio/",
	}
	for _, blocked := range blockedTypes {
		if strings.Contains(ct, blocked) {
			ext := strings.Split(ct, "/")
			typeName := "–±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª"
			if strings.Contains(ct, "pdf") {
				typeName = "PDF"
			} else if strings.Contains(ct, "image") {
				typeName = "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
			} else if strings.Contains(ct, "video") {
				typeName = "–≤–∏–¥–µ–æ"
			} else if strings.Contains(ct, "word") || strings.Contains(ct, "office") {
				typeName = "–¥–æ–∫—É–º–µ–Ω—Ç Word"
			} else if len(ext) > 1 {
				typeName = ext[1]
			}
			return "", fmt.Errorf("‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å %s.\n–ü–µ—Ä–µ–¥–∞–π—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é –∏–ª–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É (HTML), –∞ –Ω–µ –Ω–∞ —Ñ–∞–π–ª", typeName)
		}
	}

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("—Å—Ç–∞—Ç—É—Å –∫–æ–¥: %d", resp.StatusCode)
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("–æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: %w", err)
	}

	log.Printf("[FETCHER] ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ %d –±–∞–π—Ç", len(body))

	content := f.extractText(string(body))
	log.Printf("[FETCHER] ‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ %d —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞", len(content))
	if len(content) > 0 {
		log.Printf("[FETCHER] üìù –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤: %s...", truncate(content, 100))
	}

	if len(content) < 200 {
		log.Printf("[FETCHER] ‚ö† –ö–æ–Ω—Ç–µ–Ω—Ç –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π (%d —Å–∏–º–≤–æ–ª–æ–≤), –ø—Ä–æ–±—É—é —Ñ–æ–ª–±–µ–∫–∏ –¥–ª—è SPA...", len(content))

		// –§–æ–ª–±–µ–∫ 1: ld+json (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏)
		if ldContent := f.extractLdJson(string(body)); len(ldContent) >= 200 {
			log.Printf("[FETCHER] ‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ %d —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ ld+json", len(ldContent))
			return ldContent, nil
		}

		// –§–æ–ª–±–µ–∫ 2: Open Graph + meta —Ç–µ–≥–∏
		if metaContent := f.extractMetaTags(string(body)); len(metaContent) >= 50 {
			log.Printf("[FETCHER] ‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ %d —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ meta-—Ç–µ–≥–æ–≤", len(metaContent))
			return metaContent, nil
		}

		return "", fmt.Errorf("–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (%d —Å–∏–º–≤–æ–ª–æ–≤). –°–∞–π—Ç, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç JavaScript –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞", len(content))
	}

	return content, nil
}

func truncate(s string, maxLen int) string {
	runes := []rune(s)
	if len(runes) <= maxLen {
		return s
	}
	return string(runes[:maxLen])
}

// ‚îÄ‚îÄ HTML-–ø–∞—Ä—Å–µ—Ä –Ω–∞ golang.org/x/net/html ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
//
// –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ regex:
//   - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ –ª—é–±–æ–π –≥–ª—É–±–∏–Ω—ã
//   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç HTML-entities (&amp; &#x27; –∏ —Ç.–¥.)
//   - –ù–µ –ª–æ–º–∞–µ—Ç—Å—è –Ω–∞ JSX-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö —Å >, CDATA, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö
//   - –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ª–æ–º–∞–Ω–Ω—ã–π HTML (–∫–∞–∫ –±—Ä–∞—É–∑–µ—Ä)

// –¢–µ–≥–∏, —á—å—ë –ø–æ–¥–¥–µ—Ä–µ–≤–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è
var skipTags = map[string]bool{
	"script":   true,
	"style":    true,
	"noscript": true,
	"iframe":   true,
	"svg":      true,
	"canvas":   true,
	"audio":    true,
	"video":    true,
}

// –ö–ª–∞—Å—Å—ã/id —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –º—É—Å–æ—Ä (—Ä–µ–∫–ª–∞–º–∞, –Ω–∞–≤–∏–≥–∞—Ü–∏—è, –≤–∏–¥–∂–µ—Ç—ã)
var junkAttrRe = regexp.MustCompile(`(?i)\b(ad-|ads-|advert|advertisement|banner|cookie-banner|gdpr|subscribe-|newsletter|promo|popup|modal|overlay|sponsored)\b`)

// –ë–ª–æ—á–Ω—ã–µ —Ç–µ–≥–∏, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–µ–Ω –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
var blockTags = map[string]bool{
	"p": true, "h1": true, "h2": true, "h3": true, "h4": true, "h5": true, "h6": true,
	"div": true, "section": true, "article": true, "main": true,
	"blockquote": true, "li": true, "dt": true, "dd": true,
	"tr": true, "td": true, "th": true, "br": true,
	"figcaption": true,
}

// –ü–∞—Ä–∞–≥—Ä–∞—Ñ–Ω—ã–µ —Ç–µ–≥–∏ ‚Äî –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å (–Ω–æ–≤—ã–π –∞–±–∑–∞—Ü)
var paraTags = map[string]bool{
	"p": true, "h1": true, "h2": true, "h3": true, "h4": true, "h5": true, "h6": true,
	"blockquote": true, "figcaption": true,
}

func isJunkNode(n *html.Node) bool {
	// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏
	for _, attr := range n.Attr {
		switch attr.Key {
		case "class", "id":
			val := strings.ToLower(attr.Val)
			// –¢–æ–ª—å–∫–æ —è–≤–Ω–∞—è —Ä–µ–∫–ª–∞–º–∞ –∏ –ø–æ–ø–∞–ø—ã
			if strings.Contains(val, "advertisement") ||
				strings.Contains(val, "ad-banner") ||
				strings.Contains(val, "popup") ||
				strings.Contains(val, "modal") ||
				strings.Contains(val, "cookie-banner") {
				return true
			}
		case "aria-hidden":
			if attr.Val == "true" {
				return true
			}
		}
	}
	return false
}

func (f *ContentFetcher) extractText(htmlStr string) string {
	log.Printf("[FETCHER] üîç –ü–∞—Ä—Å—é HTML —á–µ—Ä–µ–∑ golang.org/x/net/html...")

	doc, err := html.Parse(strings.NewReader(htmlStr))
	if err != nil {
		log.Printf("[FETCHER] ‚ö† –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: %v", err)
		return ""
	}

	// –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ç–µ–≥–∞–º
	mainContent := f.findMainContent(doc)
	if mainContent != nil {
		log.Printf("[FETCHER] ‚úì –ù–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–∞—Ö")
		return f.extractFromNode(mainContent)
	}

	// –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ - –ø–∞—Ä—Å–∏–º –≤—Å—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
	log.Printf("[FETCHER] ‚ö† –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–∞—Ä—Å—é –≤—Å—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
	return f.extractFromNode(doc)
}

// findMainContent –∏—â–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ç–µ–≥–∞–º
func (f *ContentFetcher) findMainContent(n *html.Node) *html.Node {
	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: <article>
	if article := f.findTag(n, "article"); article != nil {
		return article
	}

	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: <main>
	if main := f.findTag(n, "main"); main != nil {
		return main
	}

	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: —ç–ª–µ–º–µ–Ω—Ç —Å –∫–ª–∞—Å—Å–æ–º/id —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "content", "article", "post", "entry"
	if content := f.findByClass(n, []string{"content", "article", "post", "entry", "main-content", "post-content"}); content != nil {
		return content
	}

	return nil
}

// findTag –∏—â–µ—Ç –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º —Ç–µ–≥–æ–º
func (f *ContentFetcher) findTag(n *html.Node, tag string) *html.Node {
	if n.Type == html.ElementNode && strings.ToLower(n.Data) == tag {
		return n
	}
	for c := n.FirstChild; c != nil; c = c.NextSibling {
		if result := f.findTag(c, tag); result != nil {
			return result
		}
	}
	return nil
}

// findByClass –∏—â–µ—Ç —ç–ª–µ–º–µ–Ω—Ç —Å –∫–ª–∞—Å—Å–æ–º/id —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –æ–¥–Ω–æ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
func (f *ContentFetcher) findByClass(n *html.Node, keywords []string) *html.Node {
	if n.Type == html.ElementNode {
		for _, attr := range n.Attr {
			if attr.Key == "class" || attr.Key == "id" {
				val := strings.ToLower(attr.Val)
				for _, keyword := range keywords {
					if strings.Contains(val, keyword) {
						return n
					}
				}
			}
		}
	}

	for c := n.FirstChild; c != nil; c = c.NextSibling {
		if result := f.findByClass(c, keywords); result != nil {
			return result
		}
	}
	return nil
}

// extractFromNode –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —É–∑–ª–∞
func (f *ContentFetcher) extractFromNode(root *html.Node) string {
	var sb strings.Builder

	var walk func(*html.Node)
	walk = func(n *html.Node) {
		if n.Type == html.ElementNode {
			tag := strings.ToLower(n.Data)

			// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –ø–æ–¥–¥–µ—Ä–µ–≤—å—è —Ü–µ–ª–∏–∫–æ–º
			if skipTags[tag] || isJunkNode(n) {
				return
			}

			// –ü–µ—Ä–µ–¥ –±–ª–æ—á–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º ‚Äî –ø–µ—Ä–µ–Ω–æ—Å (–µ—Å–ª–∏ –µ—â—ë –Ω–µ—Ç)
			if blockTags[tag] {
				s := sb.String()
				if len(s) > 0 && s[len(s)-1] != '\n' {
					sb.WriteByte('\n')
				}
			}

			// –†–µ–∫—É—Ä—Å–∏—è –≤ –¥–æ—á–µ—Ä–Ω–∏–µ —É–∑–ª—ã
			for c := n.FirstChild; c != nil; c = c.NextSibling {
				walk(c)
			}

			// –ü–æ—Å–ª–µ –±–ª–æ—á–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ ‚Äî –æ–¥–∏–Ω–∞—Ä–Ω—ã–π –∏–ª–∏ –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å
			if blockTags[tag] {
				if paraTags[tag] {
					sb.WriteString("\n\n")
				} else {
					s := sb.String()
					if len(s) == 0 || s[len(s)-1] != '\n' {
						sb.WriteByte('\n')
					}
				}
			}
			return // –¥–æ—á–µ—Ä–Ω–∏–µ —É–∂–µ –æ–±–æ—à–ª–∏ –≤—ã—à–µ
		}

		if n.Type == html.TextNode {
			text := strings.TrimSpace(n.Data)
			if text != "" {
				s := sb.String()
				// –ü—Ä–æ–±–µ–ª –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–∏–º–≤–æ–ª –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å –∏ –Ω–µ –ø—Ä–æ–±–µ–ª
				if len(s) > 0 && s[len(s)-1] != '\n' && s[len(s)-1] != ' ' {
					sb.WriteByte(' ')
				}
				sb.WriteString(text)
			}
			return
		}

		// –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ (Document, Doctype –∏ —Ç.–¥.) ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±—Ö–æ–¥–∏–º –¥–µ—Ç–µ–π
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			walk(c)
		}
	}

	walk(root)

	// ‚îÄ‚îÄ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
	spaceRe := regexp.MustCompile(`[ \t]+`)
	newlineRe := regexp.MustCompile(`\n{3,}`)

	rawLines := strings.Split(sb.String(), "\n")
	var cleanLines []string
	for _, line := range rawLines {
		line = strings.TrimSpace(line)
		line = spaceRe.ReplaceAllString(line, " ")
		if line != "" {
			cleanLines = append(cleanLines, line)
		}
	}

	text := strings.TrimSpace(newlineRe.ReplaceAllString(strings.Join(cleanLines, "\n"), "\n\n"))

	// –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
	if len([]rune(text)) > 20000 {
		runes := []rune(text)
		log.Printf("[FETCHER] ‚ö† –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (%d —Å–∏–º–≤.), –æ–±—Ä–µ–∑–∞—é –¥–æ 20000", len(runes))
		text = string(runes[:20000]) + "\n\n[...—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...]"
	}

	return text
}

// extractLdJson –∏—â–µ—Ç structured data (JSON-LD) –∏ –≤—ã—Ç—è–≥–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
func (f *ContentFetcher) extractLdJson(htmlStr string) string {
	// –ò—â–µ–º –≤—Å–µ <script type="application/ld+json">
	re := regexp.MustCompile(`(?i)<script[^>]+type=["']application/ld\+json["'][^>]*>([\s\S]*?)</script>`)
	matches := re.FindAllStringSubmatch(htmlStr, -1)

	for _, m := range matches {
		raw := strings.TrimSpace(m[1])
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(raw), &data); err != nil {
			continue
		}

		var parts []string

		if h, ok := data["headline"].(string); ok && h != "" {
			parts = append(parts, h)
		}
		if desc, ok := data["description"].(string); ok && desc != "" {
			parts = append(parts, desc)
		}
		if body, ok := data["articleBody"].(string); ok && body != "" {
			parts = append(parts, body)
		}
		if text, ok := data["text"].(string); ok && text != "" {
			parts = append(parts, text)
		}

		if len(parts) > 0 {
			result := strings.Join(parts, "\n\n")
			runes := []rune(result)
			if len(runes) > 20000 {
				result = string(runes[:20000])
			}
			return result
		}
	}
	return ""
}

// fetchFacebook fetches a public Facebook post via mbasic.facebook.com.
// mbasic serves simple HTML without JavaScript and works for public posts.
func (f *ContentFetcher) fetchFacebook(originalURL string) (string, error) {
	mbasicURL := toMbasic(originalURL)
	log.Printf("[FETCHER] üìò Facebook ‚Üí mbasic: %s", mbasicURL)

	client := &http.Client{
		Timeout: 30 * time.Second,
		// Follow redirects but stop if we land on login page
		CheckRedirect: func(req *http.Request, via []*http.Request) error {
			if strings.Contains(req.URL.String(), "/login") || strings.Contains(req.URL.String(), "login.php") {
				return fmt.Errorf("Facebook —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞")
			}
			if len(via) >= 5 {
				return fmt.Errorf("—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π")
			}
			return nil
		},
	}

	req, err := http.NewRequest("GET", mbasicURL, nil)
	if err != nil {
		return "", fmt.Errorf("–æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: %w", err)
	}

	// Mobile browser UA ‚Äî mbasic works best with mobile agents
	req.Header.Set("User-Agent", "Mozilla/5.0 (Linux; Android 12; Pixel 6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.210 Mobile Safari/537.36")
	req.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
	req.Header.Set("Accept-Language", "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7")
	req.Header.Set("Accept-Encoding", "gzip, deflate, br")

	resp, err := client.Do(req)
	if err != nil {
		// Give a friendly message if it's a login redirect
		if strings.Contains(err.Error(), "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏") {
			return "", fmt.Errorf("‚ùå Facebook –ø–æ—Å—Ç –∑–∞–∫—Ä—ã—Ç—ã–π –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –≤—Ö–æ–¥–∞ –≤ –∞–∫–∫–∞—É–Ω—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—É–±–ª–∏—á–Ω—ã–µ –ø–æ—Å—Ç—ã")
		}
		return "", fmt.Errorf("–æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Facebook: %w", err)
	}
	defer resp.Body.Close()

	log.Printf("[FETCHER] ‚úì mbasic –æ—Ç–≤–µ—Ç–∏–ª: %d", resp.StatusCode)

	// mbasic may return 302 to login ‚Äî check final URL
	if strings.Contains(resp.Request.URL.String(), "/login") {
		return "", fmt.Errorf("‚ùå Facebook —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—É–±–ª–∏—á–Ω—ã–µ –ø–æ—Å—Ç—ã")
	}

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("Facebook mbasic —Å—Ç–∞—Ç—É—Å: %d", resp.StatusCode)
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("–æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: %w", err)
	}

	log.Printf("[FETCHER] ‚úì Facebook: –∑–∞–≥—Ä—É–∂–µ–Ω–æ %d –±–∞–π—Ç", len(body))

	// Extract post text ‚Äî mbasic has a simpler DOM
	content := f.extractFacebookPost(string(body))
	if len(content) < 50 {
		// Fallback: try Open Graph meta tags (og:description contains post preview)
		content = f.extractMetaTags(string(body))
		if len(content) < 50 {
			return "", fmt.Errorf("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ Facebook. –í–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Å—Ç –ø—Ä–∏–≤–∞—Ç–Ω—ã–π –∏–ª–∏ —É–¥–∞–ª—ë–Ω")
		}
		log.Printf("[FETCHER] ‚úì Facebook: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback meta-—Ç–µ–≥–∏ (%d —Å–∏–º–≤.)", len(content))
	} else {
		log.Printf("[FETCHER] ‚úì Facebook: –∏–∑–≤–ª–µ—á–µ–Ω–æ %d —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞", len(content))
	}

	return content, nil
}

// extractFacebookPost extracts the post content from mbasic.facebook.com HTML.
// mbasic wraps post text in <div data-ft="..."> or <p> inside the story container.
func (f *ContentFetcher) extractFacebookPost(htmlStr string) string {
	doc, err := html.Parse(strings.NewReader(htmlStr))
	if err != nil {
		return ""
	}

	var parts []string
	seen := map[string]bool{}

	var walk func(*html.Node)
	walk = func(n *html.Node) {
		if n.Type == html.ElementNode {
			tag := strings.ToLower(n.Data)

			// Skip nav/header/footer/script/style
			switch tag {
			case "script", "style", "nav", "footer", "head":
				return
			}

			// mbasic wraps post body in <div data-ft> or divs with id containing "story"
			isStoryDiv := false
			for _, attr := range n.Attr {
				if attr.Key == "data-ft" {
					isStoryDiv = true
				}
				if (attr.Key == "id" || attr.Key == "class") &&
					(strings.Contains(attr.Val, "story") || strings.Contains(attr.Val, "post") || strings.Contains(attr.Val, "userContent")) {
					isStoryDiv = true
				}
			}

			if isStoryDiv {
				text := strings.TrimSpace(f.extractFromNode(n))
				if len(text) > 30 && !seen[text] {
					seen[text] = true
					parts = append(parts, text)
				}
				return // don't recurse further into this node
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			walk(c)
		}
	}
	walk(doc)

	if len(parts) == 0 {
		// Broad fallback: just extract all visible text
		return f.extractText(htmlStr)
	}

	result := strings.Join(parts, "\n\n")
	if len([]rune(result)) > 10000 {
		runes := []rune(result)
		result = string(runes[:10000])
	}
	return result
}

// extractMetaTags –≤—ã—Ç—è–≥–∏–≤–∞–µ—Ç Open Graph –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ meta-—Ç–µ–≥–∏
func (f *ContentFetcher) extractMetaTags(htmlStr string) string {
	doc, err := html.Parse(strings.NewReader(htmlStr))
	if err != nil {
		return ""
	}

	meta := map[string]string{}

	var walk func(*html.Node)
	walk = func(n *html.Node) {
		if n.Type == html.ElementNode && strings.ToLower(n.Data) == "meta" {
			var property, name, content string
			for _, attr := range n.Attr {
				switch strings.ToLower(attr.Key) {
				case "property":
					property = strings.ToLower(attr.Val)
				case "name":
					name = strings.ToLower(attr.Val)
				case "content":
					content = attr.Val
				}
			}
			key := property
			if key == "" {
				key = name
			}
			if key != "" && content != "" {
				meta[key] = content
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			walk(c)
		}
	}
	walk(doc)

	var parts []string

	// –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: og:title > twitter:title > title
	for _, key := range []string{"og:title", "twitter:title"} {
		if v, ok := meta[key]; ok {
			parts = append(parts, v)
			break
		}
	}
	// –û–ø–∏—Å–∞–Ω–∏–µ
	for _, key := range []string{"og:description", "twitter:description", "description"} {
		if v, ok := meta[key]; ok {
			parts = append(parts, v)
			break
		}
	}
	// –î–æ–ø. –ø–æ–ª—è —Å—Ç–∞—Ç—å–∏
	for _, key := range []string{"article:section", "article:tag"} {
		if v, ok := meta[key]; ok {
			parts = append(parts, v)
		}
	}

	log.Printf("[FETCHER] üìã Meta-—Ç–µ–≥–∏: title=%q desc=%q", meta["og:title"], meta["og:description"])
	return strings.TrimSpace(strings.Join(parts, "\n\n"))
}
