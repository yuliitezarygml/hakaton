# Text Analyzer (Анализатор текста)

Инструмент для проверки текстов и статей на предмет обнаружения дезинформации, манипуляций и логических ошибок.

**Три компонента:** Go Бэкенд · Next.js Фронтенд · Chrome Расширение

---

## Архитектура системы

```
┌────────────────────────────────────────────────────────────────────────┐
│                          ПОЛЬЗОВАТЕЛЬ                                  │
└─────────────────────┬──────────────────────────┬───────────────────────┘
                      │                          │
                      ▼                          ▼
      ┌───────────────────────────┐   ┌──────────────────────────────┐
      │    Chrome Расширение      │   │      Next.js Фронтенд        │
      │                           │   │      localhost:3000          │
      │  • Кнопка в тулбаре       │   │                              │
      │  • Сканирование страницы  │   │  • Поле ввода текста / URL   │
      │    с анимацией            │   │  • Живой лог событий         │
      │  • Контекстное меню       │   │  • Карточка с результатом    │
      │    (выделенный текст)     │   └───────────────┬──────────────┘
      └─────────────┬─────────────┘                   │
                    │                     /api/* proxy│
                    │   POST { url }                  │
                    └───────────────────┬──────────────┘
                                        │
                                        ▼
┌───────────────────────────────────────────────────────────────────────┐
│                       Go Бэкенд  :8080                                │
│                                                                       │
│   POST /api/analyze/stream   ──  SSE (поток событий)                  │
│   POST /api/analyze          ──  Синхронный JSON                      │
│   GET  /api/health           ──  проверка доступности                 │
│                                                                       │
│  ┌─────────────────────────────────────────────────────────────────┐  │
│  │                      Сервис Анализатора                         │  │
│  │                                                                 │  │
│  │  ШАГ 1  URL → ContentFetcher ──► скачивает HTML,                │  │
│  │                                  парсит, удаляет мусор          │  │
│  │                                  → чистый текст                 │  │
│  │                                                                 │  │
│  │  ШАГ 2  Serper API ──────────► ищет факты в Google              │  │
│  │         (если настроен)          RU + EN + RO                   │  │
│  │                                  добавляет контекст к тексу     │  │
│  │                                                                 │  │
│  │  ШАГ 3  Текст + контекст ────► AI провайдер                     │  │
│  │                                  промпт из prompts.json         │  │
│  │                                  → JSON с оценками              │  │
│  │                                                                 │  │
│  │  ШАГ 4  оценка ≤ 7 ──────────► проверка через Serper            │  │
│  │                                  3 запроса по ключевым          │  │
│  │                                  утверждениям                   │  │
│  └──────────────────────────────────┬──────────────────────────────┘  │
└─────────────────────────────────────┼───────────────────────────────┘
                                      │
               ┌──────────────────────┴────────────────────┐
               │                                           │
               ▼                                           ▼
   ┌───────────────────────┐                 ┌────────────────────────┐
   │     AI Провайдер      │                 │      Serper API        │
   │                       │                 │   (Google Search)      │
   │  • Groq               │                 │                        │
   │    llama-3.3-70b      │                 │  • Поиск фактов        │
   │  • OpenRouter         │                 │  • Языки: RU EN RO     │
   │    qwen / deepseek    │                 │  • Перекрестная        │
   │  • LM Studio          │                 │    проверка            │
   │    локальная модель   │                 └────────────────────────┘
   └───────────────────────┘
```

---

## Поток данных

```
  Запрос { text: "..." } или { url: "..." }
          │
          ▼
  ┌───────────────────────────────────────────────────────────┐
  │ ШАГ 1 — Получение текста                                  │
  │   Если URL:                                               │
  │     → HTTP GET страницы                                   │
  │     → Парсинг HTML (golang.org/x/net/html)                │
  │     → Удаляется: nav header footer aside скрипты          │
  │       реклама cookie-баннеры виджеты iframe               │
  │     → Максимум 15 000 символов                            │
  └─────────────────────────────┬─────────────────────────────┘
                                │
                                ▼
  ┌───────────────────────────────────────────────────────────┐
  │ ШАГ 2 — Поиск контекста (если задан SERPER_API_KEY)       │
  │   → 3 параллельных поисковых запроса                      │
  │   → Языки: русский + английский + румынский               │
  │   → Топ-3 результата на каждый запрос                     │
  │   → Контекст добавляется к тексту перед анализом          │
  └─────────────────────────────┬─────────────────────────────┘
                                │
                                ▼
  ┌───────────────────────────────────────────────────────────┐
  │ ШАГ 3 — Анализ через LLM                                  │
  │   → Текст + контекст → API запрос                         │
  │   → Промпт определяет алгоритм анализа из 8 шагов         │
  │   → Ответ — строгий JSON                                  │
  └─────────────────────────────┬─────────────────────────────┘
                                │
                                ▼
  ┌───────────────────────────────────────────────────────────┐
  │ ШАГ 4 — Перекрестная проверка (если оценка ≤ 7)           │
  │   → Из анализа извлекаются спорные утверждения            │
  │   → Serper ищет опровержения или подтверждения            │
  │   → Найденные источники добавляются в ответ               │
  └─────────────────────────────┬─────────────────────────────┘
                                │
                                ▼
  AnalysisResponse {
    credibility_score       — оценка 0..10
    summary                 — краткий вывод
    manipulations[]         — список найденных манипуляций
    logical_issues[]        — логические ошибки
    fact_check {
      verifiable_facts[]    — проверяемые утверждения
      opinions_as_facts[]   — мнения, выданные за факты
      missing_evidence[]    — утверждения без доказательств
    }
    verification {
      is_fake               — true / false
      fake_reasons[]        — причины
      real_information      — что найдено в интернете
      verified_sources[]    — ссылки на источники
    }
  }
```

---

## Chrome Расширение — схема работы

```
  МОДУЛЬ 1 — Кнопка в тулбаре
  ─────────────────────────────────────────────────────────────────
  Клик по иконке → открывается popup.html
          │
          ▼
  Проверка бэкенда: GET /api/health
          │
          ▼
  Клик "Анализировать" → content.js встраивается во вкладку
          │
          ▼
  Анимация content.js (3 сек):
    • Темный полупрозрачный оверлей поверх всей страницы
    • Сияющая синяя линия — зафиксирована на 35% высоты экрана
    • Страница автоматически прокручивается под ней
    • h1..h4, p, blockquote подсвечиваются при прохождении линии
    • nav / footer / реклама / cookie-баннеры — игнорируются
    • В конце страница возвращается в исходное положение
          │
          ▼
  scan_done → popup получает сигнал
          │
          ▼
  POST /api/analyze/stream { url: текущий_url }
          │
          ▼
  SSE поток → popup показывает лог в реальном времени
          │
          ▼
  event: result → карточка с оценкой 🔴 / 🟡 / 🟢 + детали


  МОДУЛЬ 2 — Контекстное меню (выделенный текст)
  ─────────────────────────────────────────────────────────────────
  Выделить текст → правый клик → "Исследовать: «...»"
          │
          ▼
  background.js (service worker):
    → сохраняет текст в chrome.storage.session
    → вызывает chrome.windows.create
          │
          ▼
  Открывается popup.html?autostart=1 (окно 440×600)
          │
          ▼
  popup.js читает pendingText → сразу:
  POST /api/analyze/stream { text: выделенный_текст }
          │
          ▼
  SSE → результат (без фазы сканирования)
```

---

## SSE События

| Событие    | Данные                            | Цвет UI         |
|------------|-----------------------------------|-----------------|
| `start`    | Начальное сообщение               | 🔵 синий        |
| `progress` | Шаг выполнения                    | 🟢 зеленый      |
| `result`   | Полный JSON с результатом         | 🟣 фиолетовый   |
| `done`     | Сообщение о завершении            | 🟠 оранжевый    |
| `error`    | Описание ошибки                   | 🔴 красный      |

---

## Шкала достоверности

| Оценка | Вердикт                            | Индикатор |
|--------|------------------------------------|-----------|
| 0 – 3  | Высокая вероятность фейка          | 🔴        |
| 4 – 6  | Сомнительный контент               | 🟡        |
| 7 – 10 | Контент выглядит достоверным       | 🟢        |

Базовая оценка — 5. Каждая манипуляция −0.5, логическая ошибка −1, факт с источником +0.5.

---

## Структура проекта

```
openrouter-web/
│
├── main.go                       # Точка входа: конфигурация, маршрутизация, запуск
│
├── config/
│   ├── config.go                 # Загрузка переменных .env
│   └── prompts.json              # Системные промпты для LLM
│
├── handlers/
│   └── analyzer.go               # HTTP обработчики (/stream, /analyze, /health)
│
├── models/
│   └── analysis.go               # Структуры запросов и ответов
│
├── services/
│   ├── analyzer.go               # Оркестратор: 4 шага анализа
│   ├── fetcher.go                # Загрузка и парсинг HTML по URL
│   ├── groq.go                   # Клиент Groq API
│   ├── openrouter.go             # Клиент OpenRouter (+ резервная модель)
│   ├── lmstudio.go               # Клиент LM Studio (локальные модели)
│   ├── serper.go                 # Клиент Google Search (Serper API)
│   └── prompt_loader.go          # Загрузка промптов из JSON
│
├── frontend/                     # Next.js 16 + React 19 + Tailwind v4
│   ├── app/
│   │   ├── page.js               # Главная страница ('use client')
│   │   ├── layout.js
│   │   └── globals.css           # Темная тема
│   ├── components/
│   │   ├── InputForm.js          # Поле ввода текста или URL
│   │   ├── EventLog.js           # Контейнер лога событий
│   │   ├── EventRow.js           # Строка лога: бейдж + текст
│   │   ├── ResultCard.js         # Карточка с результатом анализа
│   │   └── StatusBar.js          # Индикатор соединения с бэкендом
│   ├── hooks/
│   │   └── useAnalyzer.js        # SSE через fetch + ReadableStream
│   └── next.config.mjs           # Прокси /api/* → localhost:8080
│
└── extension/                    # Chrome Расширение Manifest V3
    ├── manifest.json
    ├── background.js             # Service Worker: контекстное меню
    ├── content.js                # Встраивается в страницу: анимация сканирования
    ├── popup.html                # UI: 4 состояния (ожидание/сканирование/анализ/результат)
    ├── popup.css                 # Темная тема
    └── popup.js                  # Логика: доступность → скан → SSE → результат
```

---

## Быстрый старт

### Требования

- Go 1.21+
- Node.js 18+
- Ключ API: [Groq](https://console.groq.com) (бесплатно) или OpenRouter / LM Studio

### 1. Настройка .env

```bash
cp .env.example .env
```

```env
# Groq — быстрый и бесплатный (по умолчанию)
USE_GROQ=true
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.3-70b-versatile

# OpenRouter (USE_GROQ=false)
OPENROUTER_API_KEY=sk-or-...
OPENROUTER_MODEL=qwen/qwen3-coder:free
OPENROUTER_MODEL_BACKUP=deepseek/deepseek-r1-0528:free

# LM Studio — локально, без интернета (USE_LM_STUDIO=true)
LM_STUDIO_URL=http://localhost:1234
LM_STUDIO_MODEL=local-model

# Serper — поиск фактов в Google (опционально)
SERPER_API_KEY=...

PORT=8080
```

### 2. Запуск бэкенда

```bash
go run main.go
# → http://localhost:8080
```

### 3. Запуск фронтенда

```bash
cd frontend
npm install
npm run dev
# → http://localhost:3000
```

### 4. Установка расширения

1. Открываем `chrome://extensions/`
2. Включаем **Режим разработчика**
3. **Загрузить распакованное расширение** → выбираем папку `extension/`
4. Убеждаемся, что бэкенд запущен на `localhost:8080`

---

## API

### `POST /api/analyze/stream` — SSE

```json
{ "url": "https://example.com/article" }
```
```json
{ "text": "текст для анализа..." }
```

### `POST /api/analyze` — синхронный

Те же поля ввода. Возвращает готовый JSON без стриминга.

### `GET /api/health`

```json
{ "status": "ok" }
```

---

## Провайдеры AI

| Провайдер   | Переменная             | Модель по умолчанию           | Особенности                  |
|-------------|------------------------|-------------------------------|------------------------------|
| Groq        | `USE_GROQ=true`        | llama-3.3-70b-versatile       | Бесплатно, очень быстро      |
| OpenRouter  | _(по умолчанию)_       | qwen/qwen3-coder:free         | Много моделей + failover     |
| LM Studio   | `USE_LM_STUDIO=true`   | local-model                   | Локально, без ключа API      |
